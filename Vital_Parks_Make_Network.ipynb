{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vital Parks Script Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the part of the vital parks script that builds the travel network and stores it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO! \n",
    "Add Ferries. Add City Bike. Add park paths. Add driving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the assigned walk speed in miles per hr\n",
    "walk_speed_miles_per_hr = 3.0\n",
    "\n",
    "# If taking transit we take the average transit travel times on weekdays by default.\n",
    "# If weekend_transit=True then we take the average transit travel times on weekends.\n",
    "weekend_transit = False\n",
    "\n",
    "# If taking transit we take the average transit travel times during these hours (24 hr clock)\n",
    "transit_start_hr = 7\n",
    "transit_end_hr = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import requests\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE STREET AND TRANSIT NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare street network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull and modify LION data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in lion data. It can only be read in 4000 row batches so we pull the entire dataset with a while loop\n",
    "not_finished = True\n",
    "max_object_id = 0\n",
    "data = {}\n",
    "i = 0\n",
    "url = \"\"\"https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/LION/FeatureServer/0/query\"\"\"\n",
    "while not_finished:\n",
    "    params = {\n",
    "        \"where\": \"\"\"(FeatureTyp IN ('0','6','A','C','W'))AND(NodeLevelF <> '*')AND(NodeLevelT <> '*')AND(NonPed <> 'V')AND(RW_TYPE <>' 2')AND(RW_TYPE <>'2')AND(RW_TYPE <>'11')AND(RW_TYPE <>'12')AND(RW_TYPE <>'13')AND(RW_TYPE <>'14')AND(OBJECTID>{})\"\"\".format(\n",
    "            max_object_id\n",
    "        ),\n",
    "        \"outfields\": \"\"\"OBJECTID,FeatureTyp,Street,SegmentID,RW_TYPE,NYPDID,FromLeft,ToLeft,FromRight,ToRight,XFrom,YFrom,XTo,YTo,NodeIDFrom,NodeIDTo,NodeLevelF,NodeLevelT,LBoro,RBoro,L_CD,R_CD,LCT2020,RCT2020,LCT2020Suf,RCT2020Suf,LCB2020,RCB2020,LCB2020Suf,RCB2020Suf\"\"\",\n",
    "        \"outSR\": \"4326\",\n",
    "        \"limit\": \"4000\",\n",
    "        \"f\": \"json\",\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data_current = response.json()\n",
    "    data[i] = pd.json_normalize(data_current[\"features\"])\n",
    "    max_object_id = data_current[\"features\"][-1][\"attributes\"][\"OBJECTID\"]\n",
    "    if len(data[i]) < 4000:\n",
    "        not_finished = False\n",
    "    i = i + 1\n",
    "lion = pd.concat(data).reset_index(drop=True)\n",
    "lion.columns = lion.columns.str.lstrip(\"attributes.\")\n",
    "lion = lion.rename(columns={\"geometry.paths\": \"geometry\"})\n",
    "lion[\"geometry\"] = lion[\"geometry\"].apply(lambda x: LineString(x[0]))\n",
    "lion = gpd.GeoDataFrame(lion, geometry=\"geometry\", crs=4326).to_crs(2263)\n",
    "\n",
    "# Make sure source and target nodes include node-level as part of its unique ID so we keep track of how roads actually connect accounting for both location and level\n",
    "lion[\"source\"] = lion[\"NodeIDFrom\"] + lion[\"NodeLevelF\"]\n",
    "lion[\"target\"] = lion[\"NodeIDTo\"] + lion[\"NodeLevelT\"]\n",
    "\n",
    "# Change names of From/To so it matches our Source/Target vocabulary as we keep track of lat/lon coordinates for each node\n",
    "lion = lion.rename(\n",
    "    columns={\n",
    "        \"XFrom\": \"XCoord_source\",\n",
    "        \"YFrom\": \"YCoord_source\",\n",
    "        \"XTo\": \"XCoord_target\",\n",
    "        \"YTo\": \"YCoord_target\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert walk speed to feet since we will be using crs=2263 for this script which measures distance in feet\n",
    "walk_speed_feet_per_second = walk_speed_miles_per_hr * 1.46667\n",
    "\n",
    "# Group by segment ID so segmentID becomes a unique ID.\n",
    "# This is needed because there are times when multiple rows have the same SegmentID but in these cases they are actually representing the same physical segment\n",
    "# These duplications exist for to other Lion dataset uses that we do not need for this instance.\n",
    "lion_walkable = (\n",
    "    lion.groupby(\"SegmentID\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"source\": \"min\",\n",
    "            \"target\": \"max\",\n",
    "            \"Street\": \"first\",\n",
    "            \"FeatureTyp\": \"min\",\n",
    "            \"RW_TYPE\": \"min\",\n",
    "            \"FromLeft\": \"min\",\n",
    "            \"ToLeft\": \"max\",\n",
    "            \"FromRight\": \"min\",\n",
    "            \"ToRight\": \"max\",\n",
    "            \"NYPDID\": \"first\",\n",
    "            \"LBoro\": \"first\",\n",
    "            \"RBoro\": \"first\",\n",
    "            \"L_CD\": \"first\",\n",
    "            \"R_CD\": \"first\",\n",
    "            \"LCT2020\": \"first\",\n",
    "            \"RCT2020\": \"first\",\n",
    "            \"LCT2020Suf\": \"first\",\n",
    "            \"RCT2020Suf\": \"first\",\n",
    "            \"LCB2020\": \"first\",\n",
    "            \"RCB2020\": \"first\",\n",
    "            \"LCB2020Suf\": \"first\",\n",
    "            \"RCB2020Suf\": \"first\",\n",
    "            \"XCoord_source\": \"mean\",\n",
    "            \"YCoord_source\": \"mean\",\n",
    "            \"XCoord_target\": \"mean\",\n",
    "            \"YCoord_target\": \"mean\",\n",
    "            \"geometry\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "lion_walkable = gpd.GeoDataFrame(lion_walkable, geometry=\"geometry\", crs=2263)\n",
    "\n",
    "# Assign each segment a weight equal to the number of seconds required to travel (walk) each segment\n",
    "lion_walkable[\"length\"] = lion_walkable[\"geometry\"].apply(lambda x: float(x.length))\n",
    "lion_walkable[\"weight\"] = lion_walkable[\"length\"] / walk_speed_feet_per_second\n",
    "lion_walkable[\"mode\"] = \"walk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up each street centerline into its left/right block.\n",
    "# Assign each block a unique idea by its segmentID + (L or R)\n",
    "\n",
    "# Get left blocks from street centerline data\n",
    "lion_walkable_left = lion_walkable[\n",
    "    [\n",
    "        \"SegmentID\",\n",
    "        \"Street\",\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"weight\",\n",
    "        \"mode\",\n",
    "        \"FeatureTyp\",\n",
    "        \"RW_TYPE\",\n",
    "        \"FromLeft\",\n",
    "        \"ToLeft\",\n",
    "        \"length\",\n",
    "        \"NYPDID\",\n",
    "        \"LBoro\",\n",
    "        \"L_CD\",\n",
    "        \"LCT2020\",\n",
    "        \"LCT2020Suf\",\n",
    "        \"LCB2020\",\n",
    "        \"LCB2020Suf\",\n",
    "        \"XCoord_source\",\n",
    "        \"YCoord_source\",\n",
    "        \"XCoord_target\",\n",
    "        \"YCoord_target\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]\n",
    "lion_walkable_left = lion_walkable_left.rename(\n",
    "    columns={\n",
    "        \"FromLeft\": \"From\",\n",
    "        \"ToLeft\": \"To\",\n",
    "        \"LBoro\": \"Boro\",\n",
    "        \"L_CD\": \"CommunityBoard\",\n",
    "        \"LCT2020\": \"CT2020\",\n",
    "        \"LCT2020Suf\": \"CT2020Suf\",\n",
    "        \"LCB2020\": \"CB2020\",\n",
    "        \"LCB2020Suf\": \"CB2020Suf\",\n",
    "    }\n",
    ")\n",
    "lion_walkable_left[\"SideOfStreet\"] = \"L\"\n",
    "lion_walkable_left[\"uniqueID\"] = lion_walkable_left[\"SegmentID\"] + \"L\"\n",
    "\n",
    "# Get right blocks from street centerline data\n",
    "# We flip the order of source/target nodes for the right blocks. This is just a way to make sure each street segment in our network can be walked in both directions.\n",
    "lion_walkable_right = lion_walkable[\n",
    "    [\n",
    "        \"SegmentID\",\n",
    "        \"Street\",\n",
    "        \"source\",\n",
    "        \"target\",\n",
    "        \"weight\",\n",
    "        \"mode\",\n",
    "        \"FeatureTyp\",\n",
    "        \"RW_TYPE\",\n",
    "        \"FromRight\",\n",
    "        \"ToRight\",\n",
    "        \"length\",\n",
    "        \"NYPDID\",\n",
    "        \"RBoro\",\n",
    "        \"R_CD\",\n",
    "        \"RCT2020\",\n",
    "        \"RCT2020Suf\",\n",
    "        \"RCB2020\",\n",
    "        \"RCB2020Suf\",\n",
    "        \"XCoord_source\",\n",
    "        \"YCoord_source\",\n",
    "        \"XCoord_target\",\n",
    "        \"YCoord_target\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]\n",
    "lion_walkable_right = lion_walkable_right.rename(\n",
    "    columns={\n",
    "        \"source\": \"target\",\n",
    "        \"target\": \"source\",\n",
    "        \"FromRight\": \"From\",\n",
    "        \"ToRight\": \"To\",\n",
    "        \"RBoro\": \"Boro\",\n",
    "        \"R_CD\": \"CommunityBoard\",\n",
    "        \"RCT2020\": \"CT2020\",\n",
    "        \"RCT2020Suf\": \"CT2020Suf\",\n",
    "        \"RCB2020\": \"CB2020\",\n",
    "        \"RCB2020Suf\": \"CB2020Suf\",\n",
    "        \"XCoord_source\": \"XCoord_target\",\n",
    "        \"YCoord_source\": \"YCoord_target\",\n",
    "        \"XCoord_target\": \"XCoord_source\",\n",
    "        \"YCoord_target\": \"YCoord_source\",\n",
    "    }\n",
    ")\n",
    "lion_walkable_right[\"SideOfStreet\"] = \"R\"\n",
    "lion_walkable_right[\"uniqueID\"] = lion_walkable_right[\"SegmentID\"] + \"R\"\n",
    "\n",
    "# Concat left and right blocks together to get all blocks throughout the city included in our walkable network\n",
    "lion_walkable = pd.concat([lion_walkable_left, lion_walkable_right])\n",
    "\n",
    "# For each source-target pair we make an edge label in the format of networkX as we will compare these edges to network edges soon.\n",
    "lion_walkable[\"edges\"] = list(zip(lion_walkable[\"source\"], lion_walkable[\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect LION walkable streets to OSM park paths (To Do in future version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lion data into networkx data structure so we can remove small disconnected areas that are not part of this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network graph in networkX from our lion_walkable dataframe\n",
    "Walk_DG = nx.from_pandas_edgelist(lion_walkable, create_using=nx.DiGraph())\n",
    "\n",
    "# Remove all but largest two connected components (Walkably connected NYC + Walkably connected Staten Island)\n",
    "Walk_DG = Walk_DG.subgraph(\n",
    "    [\n",
    "        p\n",
    "        for ps in sorted(\n",
    "            list(nx.strongly_connected_components(Walk_DG)), key=len, reverse=True\n",
    "        )[:2]\n",
    "        for p in ps\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Restrict our lion_walkable table to only include edges in one of these two largest connected components\n",
    "lion_walkable = lion_walkable[lion_walkable[\"edges\"].isin(list(Walk_DG.edges))]\n",
    "lion_walkable = lion_walkable.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Lion Data to Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a few lion streets that have no census geography info. We keep these streets for potential walking uses but will assign them no population.\n",
    "lion_walkable.loc[lion_walkable[\"Boro\"].isna(), \"Boro\"] = 0\n",
    "\n",
    "\n",
    "# This function converts the hierarchical census codes of county (borough), census-tract, and census-block to a single census 'geo_ID'\n",
    "def create_census_GeoID(lion_in):\n",
    "    Boro_Code = lion_in[\"Boro\"].astype(int).astype(str)\n",
    "    Boro_Code.loc[Boro_Code == \"1\"] = \"061\"\n",
    "    Boro_Code.loc[Boro_Code == \"2\"] = \"005\"\n",
    "    Boro_Code.loc[Boro_Code == \"3\"] = \"047\"\n",
    "    Boro_Code.loc[Boro_Code == \"4\"] = \"081\"\n",
    "    Boro_Code.loc[Boro_Code == \"5\"] = \"085\"\n",
    "    Tract = lion_in[\"CT2020\"].str.replace(\" \", \"0\")\n",
    "    Tract_Suf = lion_in[\"CT2020Suf\"].str.replace(\" \", \"0\")\n",
    "    Block_Code = lion_in[\"CB2020\"].str[0]\n",
    "\n",
    "    geo_ID = Boro_Code + Tract + Tract_Suf + Block_Code\n",
    "\n",
    "    return geo_ID\n",
    "\n",
    "\n",
    "# Create GEO_ID from each blocks geographical data so that we merge census data to city blocks\n",
    "lion_walkable[\"Geo_ID\"] = create_census_GeoID(lion_walkable)\n",
    "\n",
    "\n",
    "# Assign 0 'livable length' to blocks that have no addresses on them. We will then proportionally assign census block populations according to each city block's livable length.\n",
    "lion_walkable[\"livable_length\"] = lion_walkable[\"length\"]\n",
    "lion_walkable.loc[\n",
    "    ((lion_walkable[\"From\"] == 0) & (lion_walkable[\"To\"] == 0)), \"livable_length\"\n",
    "] = 0\n",
    "lion_walkable.loc[lion_walkable[\"Geo_ID\"].isna(), \"livable_length\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format 2020 census population data to join to lion dataset\n",
    "url = \"\"\"https://api.census.gov/data/2020/dec/dhc?get=group(P1)&for=block%20group:*&in=state:36&in=county:061,005,047,081,085&in=tract:*\"\"\"\n",
    "response = requests.get(url)\n",
    "Census_data = response.json()\n",
    "Census_data = pd.DataFrame(Census_data[1:], columns=Census_data[0])\n",
    "Census_data[\"GEO_ID\"] = Census_data[\"GEO_ID\"].str[-10:]\n",
    "Census_data = Census_data[[\"GEO_ID\", \"P1_001N\"]].rename(\n",
    "    columns={\"GEO_ID\": \"Geo_ID\", \"P1_001N\": \"BG_total_pop\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign census-block population data to city blocks proportional to each city-blocks livable length\n",
    "\n",
    "# Calculate the proportion of livable length that each city-block makes up for in its respective census-block-group\n",
    "total_liveable_lengths = (\n",
    "    lion_walkable[[\"Geo_ID\", \"livable_length\"]]\n",
    "    .groupby(\"Geo_ID\")\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"livable_length\": \"Total_geo_ID_livable_length\"})\n",
    ")\n",
    "lion_walkable = lion_walkable.merge(total_liveable_lengths, how=\"left\", on=\"Geo_ID\")\n",
    "lion_walkable.loc[:, \"Proportion_of_blockgroup\"] = lion_walkable[\"livable_length\"] / (\n",
    "    lion_walkable[\"Total_geo_ID_livable_length\"] + 0.00001\n",
    ")\n",
    "\n",
    "# Merge with census data and assign population from each census block group to each city block according to the above calculated proportion\n",
    "lion_walkable = lion_walkable.merge(Census_data, how=\"left\", on=\"Geo_ID\")\n",
    "lion_walkable.loc[lion_walkable[\"BG_total_pop\"].isna(), \"BG_total_pop\"] = 0\n",
    "lion_walkable[\"BG_total_pop\"] = lion_walkable[\"BG_total_pop\"].astype(float)\n",
    "lion_walkable[\"population\"] = (\n",
    "    lion_walkable[\"Proportion_of_blockgroup\"] * lion_walkable[\"BG_total_pop\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign Lion streets to the geographical regions of interest that are not already in lion features data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to City Council Districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and formate council district boundaries\n",
    "\n",
    "url = \"\"\"https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_City_Council_Districts/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson\"\"\"\n",
    "council_district_shapes = gpd.read_file(url, params=params)\n",
    "council_district_shapes = council_district_shapes[[\"CounDist\", \"geometry\"]].to_crs(2263)\n",
    "council_district_shapes = council_district_shapes.rename(\n",
    "    columns={\"CounDist\": \"Council_District\"}\n",
    ")\n",
    "council_district_shapes[\"Council_District\"] = council_district_shapes[\n",
    "    \"Council_District\"\n",
    "].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign city blocks to council districts based on geographic data. (This is an estimate and could be made more precise with some geographical research.)\n",
    "\n",
    "council_near = lion_walkable.loc[:, [\"uniqueID\", \"geometry\"]].sjoin_nearest(\n",
    "    council_district_shapes, how=\"left\", distance_col=\"dis\"\n",
    ")\n",
    "council_near = council_near.sort_values(\n",
    "    \"Council_District\", ascending=False\n",
    ").sort_values(\"dis\", ascending=False)\n",
    "council_near = council_near.groupby(\"uniqueID\").first().reset_index()\n",
    "council_near = council_near[[\"uniqueID\", \"Council_District\"]]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(council_near, on=\"uniqueID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to yes/no state designated disadvantaged communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and formate disadvantaged community census tracts\n",
    "\n",
    "url = \"\"\"https://data.ny.gov/resource/2e6c-s6fp.geojson?$limit=10000&$where=nyc_region='NYC' \"\"\"\n",
    "DAC_shapes = gpd.read_file(url)\n",
    "\n",
    "DAC_shapes = DAC_shapes[[\"geoid\", \"dac_designation\", \"geometry\"]].to_crs(2263)\n",
    "DAC_shapes[\"geoid\"] = DAC_shapes[\"geoid\"].str[2:]\n",
    "DAC_shapes = DAC_shapes.rename(\n",
    "    columns={\"geoid\": \"DAC_ID\", \"dac_designation\": \"DAC_Designation\"}\n",
    ")\n",
    "\n",
    "DAC_shapes = DAC_shapes.dissolve(by=\"DAC_ID\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First try to match on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match most blocks to this set of DAC tracts based on their Geo_ID.\n",
    "\n",
    "lion_walkable[\"DAC_ID\"] = lion_walkable[\"Geo_ID\"].str[:-1]\n",
    "lion_walkable = lion_walkable.merge(\n",
    "    DAC_shapes[[\"DAC_ID\", \"DAC_Designation\"]], how=\"left\", on=\"DAC_ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Match remaining unaccounted for streets with geo-spacial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most DAC tracts match to our Geo_IDs but a few of them dont match perfectly probably for some census tract details I cant perfectly answer.\n",
    "# Due to this small issue with matching DAC_IDs to Geo_IDs we match the remaining blocks to their nearest DAC/NotDAC census tract based on geography.\n",
    "# Note that this step is also an approximation and could be improved with some geographical research\n",
    "\n",
    "DAC_near = (\n",
    "    lion_walkable[lion_walkable[\"DAC_Designation\"].isna()]\n",
    "    .loc[:, [\"uniqueID\", \"geometry\"]]\n",
    "    .sjoin_nearest(DAC_shapes, how=\"left\", distance_col=\"dis\")\n",
    ")\n",
    "DAC_near = DAC_near.sort_values(\"DAC_Designation\", ascending=False).sort_values(\n",
    "    \"dis\", ascending=False\n",
    ")\n",
    "DAC_near = DAC_near.groupby(\"uniqueID\").first().reset_index()\n",
    "DAC_near = DAC_near[[\"uniqueID\", \"DAC_ID\", \"DAC_Designation\"]].rename(\n",
    "    columns={\"DAC_ID\": \"DAC_ID_geom\", \"DAC_Designation\": \"DAC_Designation_geom\"}\n",
    ")\n",
    "\n",
    "lion_walkable = lion_walkable.merge(DAC_near, on=\"uniqueID\", how=\"left\")\n",
    "lion_walkable.loc[lion_walkable[\"DAC_Designation\"].isna(), \"DAC_ID\"] = (\n",
    "    lion_walkable.loc[lion_walkable[\"DAC_Designation\"].isna(), \"DAC_ID_geom\"]\n",
    ")\n",
    "lion_walkable.loc[lion_walkable[\"DAC_Designation\"].isna(), \"DAC_Designation\"] = (\n",
    "    lion_walkable.loc[lion_walkable[\"DAC_Designation\"].isna(), \"DAC_Designation_geom\"]\n",
    ")\n",
    "lion_walkable = lion_walkable.drop(columns=[\"DAC_ID_geom\", \"DAC_Designation_geom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign lion streets to Gun Violence Prevention police precincts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format Police Precinct boundaries\n",
    "\n",
    "url = \"\"\"https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Police_Precincts/FeatureServer/0/query?where=1%3D1&objectIds=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&relationParam=&returnGeodetic=false&outFields=&returnGeometry=true&returnCentroid=false&returnEnvelope=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&defaultSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&collation=&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnTrueCurves=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\"\"\n",
    "\n",
    "precinct_shapes = gpd.read_file(url)\n",
    "precinct_shapes = precinct_shapes[[\"Precinct\", \"geometry\"]].to_crs(2263)\n",
    "precinct_shapes.loc[\n",
    "    precinct_shapes[\"Precinct\"].isin([40, 42, 44, 47, 73, 75]), \"GVP\"\n",
    "] = \"Yes GVP\"\n",
    "precinct_shapes.loc[\n",
    "    ~precinct_shapes[\"Precinct\"].isin([40, 42, 44, 47, 73, 75]), \"GVP\"\n",
    "] = \"No GVP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign blocks to their police precinct by geography. This is an approximation that could be improved with geographical research.\n",
    "\n",
    "precinct_near = lion_walkable.loc[:, [\"uniqueID\", \"geometry\"]].sjoin_nearest(\n",
    "    precinct_shapes, how=\"left\", distance_col=\"dis\"\n",
    ")\n",
    "precinct_near = precinct_near.sort_values(\"Precinct\", ascending=False).sort_values(\n",
    "    \"dis\", ascending=False\n",
    ")\n",
    "precinct_near = precinct_near.groupby(\"uniqueID\").first().reset_index()\n",
    "precinct_near = precinct_near[[\"uniqueID\", \"Precinct\", \"GVP\"]]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(precinct_near, on=\"uniqueID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Lion Data to TRIE communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and format TRIE community boundaries.\n",
    "\n",
    "url = \"\"\"https://services3.arcgis.com/xJHn8F2NTtwCMFtX/ArcGIS/rest/services/TRIE/FeatureServer/0/query\"\"\"\n",
    "\n",
    "params = {\"where\": \"1=1\", \"outfields\": \"*\", \"f\": \"json\"}\n",
    "response = requests.get(url, params=params)\n",
    "Trie_shapes = response.json()\n",
    "Trie_shapes = pd.json_normalize(Trie_shapes[\"features\"])\n",
    "for row in Trie_shapes.index:\n",
    "    Trie_shapes.loc[row, \"geometry\"] = Polygon(\n",
    "        Trie_shapes.loc[row, \"geometry.rings\"][0]\n",
    "    )\n",
    "Trie_shapes = Trie_shapes[\n",
    "    [\"attributes.OBJECTID\", \"attributes.Neighborhood\", \"geometry\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"attributes.OBJECTID\": \"Trie_label\",\n",
    "        \"attributes.Neighborhood\": \"Trie_name\",\n",
    "    }\n",
    ")\n",
    "Trie_shapes = gpd.GeoDataFrame(Trie_shapes, geometry=\"geometry\", crs=2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign city blocks to TRIE communities based on geographical approximations. Perhaps can be improved with more geographical research.\n",
    "\n",
    "trie_intersections = gpd.overlay(\n",
    "    lion_walkable[[\"geometry\", \"uniqueID\", \"length\"]], Trie_shapes\n",
    ")\n",
    "trie_intersections[\"len\"] = trie_intersections.length\n",
    "trie_intersections = trie_intersections.sort_values(\"len\", ascending=False)\n",
    "trie_intersections = trie_intersections.groupby(\"uniqueID\").first().reset_index()\n",
    "trie_intersections.loc[\n",
    "    trie_intersections[\"len\"] < (0.3 * trie_intersections[\"length\"]), \"Trie_label\"\n",
    "] = 0\n",
    "trie_intersections.loc[\n",
    "    trie_intersections[\"len\"] < (0.3 * trie_intersections[\"length\"]), \"Trie_name\"\n",
    "] = \"Not Trie\"\n",
    "trie_intersections = trie_intersections[[\"uniqueID\", \"Trie_label\", \"Trie_name\"]]\n",
    "\n",
    "lion_walkable = lion_walkable.merge(trie_intersections, on=\"uniqueID\", how=\"left\")\n",
    "lion_walkable.loc[lion_walkable[\"Trie_label\"].isna(), \"Trie_label\"] = 0\n",
    "lion_walkable.loc[lion_walkable[\"Trie_name\"].isna(), \"Trie_name\"] = \"Not Trie\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Walkable_DG graph with full lion_walkable atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graph based on these city-block edges and store each blocks geographical and population data as edge attributes\n",
    "\n",
    "Walk_DG = nx.from_pandas_edgelist(\n",
    "    lion_walkable,\n",
    "    create_using=nx.DiGraph(),\n",
    "    edge_attr=[\n",
    "        \"uniqueID\",\n",
    "        \"Street\",\n",
    "        \"length\",\n",
    "        \"geometry\",\n",
    "        \"SegmentID\",\n",
    "        \"XCoord_source\",\n",
    "        \"YCoord_source\",\n",
    "        \"XCoord_target\",\n",
    "        \"YCoord_target\",\n",
    "        \"RW_TYPE\",\n",
    "        \"FeatureTyp\",\n",
    "        \"NYPDID\",\n",
    "        \"SideOfStreet\",\n",
    "        \"From\",\n",
    "        \"To\",\n",
    "        \"Boro\",\n",
    "        \"CommunityBoard\",\n",
    "        \"weight\",\n",
    "        \"mode\",\n",
    "        \"CT2020\",\n",
    "        \"CT2020Suf\",\n",
    "        \"CB2020\",\n",
    "        \"CB2020Suf\",\n",
    "        \"Geo_ID\",\n",
    "        \"livable_length\",\n",
    "        \"Total_geo_ID_livable_length\",\n",
    "        \"Proportion_of_blockgroup\",\n",
    "        \"BG_total_pop\",\n",
    "        \"population\",\n",
    "        \"DAC_ID\",\n",
    "        \"DAC_Designation\",\n",
    "        \"Precinct\",\n",
    "        \"GVP\",\n",
    "        \"Trie_label\",\n",
    "        \"Trie_name\",\n",
    "        \"Council_District\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also store geographic location information (lat/lon) for all of our nodes (segment connections)\n",
    "\n",
    "lion_walkable_nodes = pd.concat(\n",
    "    [\n",
    "        lion_walkable[[\"source\", \"XCoord_source\", \"YCoord_source\"]].rename(\n",
    "            columns={\"XCoord_source\": \"x\", \"YCoord_source\": \"y\", \"source\": \"NodeID\"}\n",
    "        ),\n",
    "        lion_walkable[[\"target\", \"XCoord_target\", \"YCoord_target\"]].rename(\n",
    "            columns={\"XCoord_target\": \"x\", \"YCoord_target\": \"y\", \"target\": \"NodeID\"}\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "lion_walkable_nodes = (\n",
    "    lion_walkable_nodes.drop_duplicates(subset=\"NodeID\")\n",
    "    .set_index(\"NodeID\")\n",
    "    .apply(lambda x: Point(x[\"x\"], x[\"y\"]), axis=1)\n",
    ")\n",
    "\n",
    "nx.set_node_attributes(Walk_DG, lion_walkable_nodes.to_dict(), name=\"geometry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Transit network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import stops and stoptime gtfs data for MTA subways and buses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in regular schedule MTA data (This data gets updated every 3 months-ish)\n",
    "\n",
    "\n",
    "def gtfs_mta_import(url):\n",
    "    response = requests.get(url)\n",
    "    file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    stops_out = pd.read_csv(file.open(\"stops.txt\"))[\n",
    "        [\"stop_id\", \"stop_name\", \"stop_lat\", \"stop_lon\"]\n",
    "    ]\n",
    "    stops_out[\"stop_id\"] = stops_out[\"stop_id\"].astype(str)\n",
    "    stop_times_out = pd.read_csv(file.open(\"stop_times.txt\"))[\n",
    "        [\"trip_id\", \"stop_id\", \"arrival_time\", \"departure_time\", \"stop_sequence\"]\n",
    "    ]\n",
    "    stop_times_out[\"stop_id\"] = stop_times_out[\"stop_id\"].astype(str)\n",
    "    return stops_out, stop_times_out\n",
    "\n",
    "\n",
    "# We include city bus, special bus, and subway data. LIRR and other transit data can also be found on the MTA developer page if one is interested\n",
    "bx_bus_stops, bx_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_bx.zip\"\"\"\n",
    ")\n",
    "bk_bus_stops, bk_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_b.zip\"\"\"\n",
    ")\n",
    "mn_bus_stops, mn_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_m.zip\"\"\"\n",
    ")\n",
    "qn_bus_stops, qn_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_q.zip\"\"\"\n",
    ")\n",
    "si_bus_stops, si_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_si.zip\"\"\"\n",
    ")\n",
    "exp_bus_stops, exp_bus_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_busco.zip\"\"\"\n",
    ")\n",
    "subway_stops, subway_stop_times = gtfs_mta_import(\n",
    "    \"\"\"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_subway.zip\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format stop locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\"https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary_Water_Included/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson\"\"\"\n",
    "response = requests.get(url)\n",
    "boros = gpd.GeoDataFrame.from_features(response.json()).set_crs(4326).to_crs(2263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format stops data\n",
    "def format_mta_stops_data(stops_in):\n",
    "    stops_in[\"geometry\"] = stops_in.apply(\n",
    "        lambda x: Point(x[\"stop_lon\"], x[\"stop_lat\"]), axis=1\n",
    "    )\n",
    "    stops_in = gpd.GeoDataFrame(stops_in, geometry=\"geometry\", crs=\"epsg:4326\").to_crs(\n",
    "        \"epsg:2263\"\n",
    "    )\n",
    "    stops_in[\"stop_id\"] = stops_in[\"stop_id\"].apply(str)\n",
    "    return stops_in\n",
    "\n",
    "\n",
    "stops = pd.concat(\n",
    "    [\n",
    "        bx_bus_stops,\n",
    "        bk_bus_stops,\n",
    "        mn_bus_stops,\n",
    "        qn_bus_stops,\n",
    "        si_bus_stops,\n",
    "        exp_bus_stops,\n",
    "        subway_stops,\n",
    "    ]\n",
    ")\n",
    "stops = format_mta_stops_data(stops)\n",
    "\n",
    "# Only include stops within the city limits\n",
    "city = (\n",
    "    gpd.GeoDataFrame([boros.buffer(100).unary_union])\n",
    "    .rename(columns={0: \"geometry\"})\n",
    "    .set_geometry(col=\"geometry\", crs=2263)\n",
    ")\n",
    "stops = gpd.sjoin(stops, city, how=\"left\")\n",
    "stops = stops.loc[~stops[\"index_right\"].isna(), [\"stop_id\", \"stop_name\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges that connect each stop to its nearest level M (ground level) street node.\n",
    "# Add a weight for how long it takes to walk from street node to stop node or vice versa.\n",
    "\n",
    "lion_street_nodes_gdf = (\n",
    "    gpd.GeoDataFrame(lion_walkable_nodes)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"geometry\"})\n",
    "    .set_geometry(col=\"geometry\", crs=2263)\n",
    ")\n",
    "lion_street_nodes_gdf = lion_street_nodes_gdf[\n",
    "    lion_street_nodes_gdf[\"NodeID\"].str[-1] == \"M\"\n",
    "]\n",
    "stop_node_connections = gpd.sjoin_nearest(\n",
    "    stops, lion_street_nodes_gdf, how=\"left\", distance_col=\"dist\"\n",
    ")\n",
    "stop_node_connections = stop_node_connections.sort_values(\"dist\")\n",
    "stop_node_connections = (\n",
    "    stop_node_connections.groupby([\"stop_id\", \"NodeID\"])\n",
    "    .agg({\"stop_name\": \"first\", \"dist\": \"min\"})\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we can both enter and exit every stop by swapping the source and target and concatenating the resulting dataframes\n",
    "\n",
    "stop_node_connections[\"stop_id\"] = stop_node_connections[\"stop_id\"] + \"_mta\"\n",
    "stop_node_connections[\"mode\"] = \"walk_mta_connection\"\n",
    "stop_node_connections[\"weight\"] = (\n",
    "    stop_node_connections[\"dist\"] / walk_speed_feet_per_second\n",
    ")\n",
    "stop_node_connections_exit = stop_node_connections.rename(\n",
    "    columns={\"stop_id\": \"source\", \"NodeID\": \"target\"}\n",
    ")\n",
    "stop_node_connections_enter = stop_node_connections.rename(\n",
    "    columns={\"stop_id\": \"target\", \"NodeID\": \"source\"}\n",
    ")\n",
    "stop_node_connections = pd.concat(\n",
    "    [stop_node_connections_enter, stop_node_connections_exit]\n",
    ")\n",
    "\n",
    "# Create a networkX network made of of edges that connect each MTA stop to its nearest street level node\n",
    "walk_to_MTA_DG = nx.from_pandas_edgelist(\n",
    "    stop_node_connections,\n",
    "    create_using=nx.DiGraph(),\n",
    "    edge_attr=[\"stop_name\", \"weight\", \"mode\"],\n",
    ")\n",
    "\n",
    "# Stitch the walkable street network and the enter/exit MTA stop locations network together\n",
    "complete_DG = nx.compose(Walk_DG, walk_to_MTA_DG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate travel times (including expected wait times) between connected stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code chunk calculates the average travel time between any connected pair of MTA stops.\n",
    "# This average is with respect to the times specified by our initial parameters:\n",
    "##  weekend_transit (True/False), transit_start_hr (INT btwn 0-24), transit_end_hr (INT btwn transit_start_hr-24)\n",
    "### (Intuitively, we are calculating the average (fastest possible travel time between each feasible stop-pair\n",
    "###  where this average is over all possible trip start times that fall between our set transit_start_hr and transit_end_hr parameters.)\n",
    "\n",
    "# Format stop-times data\n",
    "stop_times = pd.concat(\n",
    "    [\n",
    "        bx_bus_stop_times,\n",
    "        bk_bus_stop_times,\n",
    "        mn_bus_stop_times,\n",
    "        qn_bus_stop_times,\n",
    "        si_bus_stop_times,\n",
    "        exp_bus_stop_times,\n",
    "        subway_stop_times,\n",
    "    ]\n",
    ")\n",
    "stop_times[\"stop_id\"] = stop_times[\"stop_id\"].astype(str)\n",
    "stop_times[\"stop_id\"] = stop_times[\"stop_id\"] + \"_mta\"\n",
    "\n",
    "# Remove any stops that are not part of our walkable-street to MTA stop connected network. (e.g. not in city boundaries like Hoboken...)\n",
    "stop_times = stop_times[\n",
    "    stop_times[\"stop_id\"].isin(stop_node_connections_exit[\"source\"].unique())\n",
    "]\n",
    "\n",
    "# If weekend_transit is set to false then only include weekday trips\n",
    "if weekend_transit == False:\n",
    "    trips = stop_times[stop_times[\"trip_id\"].str.contains(\"Weekday\")]\n",
    "\n",
    "# Only consider trips between our chosen start and end times\n",
    "trips = trips.loc[\n",
    "    (trips[\"departure_time\"] >= str(transit_start_hr).zfill(2))\n",
    "    & (trips[\"departure_time\"] <= str(transit_end_hr).zfill(2))\n",
    "]\n",
    "\n",
    "# Create the collection of pairwise stop-to-stop connections.\n",
    "## (We take this more detailed approach of stop-to-stop edges instead of the previous route based approach because we want to account for the 14th to w4 situation.\n",
    "##  In this situation you would happily take either the A, C, or E train, which ever comes first.\n",
    "##  So when you arrive a at 14th street you are not waiting for any MTA route one but for the first of three possible MTA routes that arrives.)\n",
    "\n",
    "# Connect any two stops that are part of a shared route.\n",
    "departures = trips[[\"trip_id\", \"departure_time\", \"stop_id\", \"stop_sequence\"]].rename(\n",
    "    columns={\"stop_id\": \"departure_stop\", \"stop_sequence\": \"dep_sequence\"}\n",
    ")\n",
    "arrivals = trips[[\"trip_id\", \"arrival_time\", \"stop_id\", \"stop_sequence\"]].rename(\n",
    "    columns={\"stop_id\": \"arrival_stop\", \"stop_sequence\": \"arr_sequence\"}\n",
    ")\n",
    "stop_to_stop = departures.merge(arrivals, on=\"trip_id\", how=\"left\")\n",
    "\n",
    "# Stop connections are directed so only include the situations where the departure stop comes before the arrival stop in that route\n",
    "stop_to_stop = stop_to_stop[stop_to_stop[\"dep_sequence\"] < stop_to_stop[\"arr_sequence\"]]\n",
    "\n",
    "# Provide a unique label for each feasible departure_stop-to-arrival_stop pairing\n",
    "stop_to_stop[\"stop_pair\"] = (\n",
    "    stop_to_stop[\"departure_stop\"].astype(str)\n",
    "    + \"_\"\n",
    "    + stop_to_stop[\"arrival_stop\"].astype(str)\n",
    ")\n",
    "stop_to_stop = stop_to_stop[\n",
    "    [\n",
    "        \"trip_id\",\n",
    "        \"departure_time\",\n",
    "        \"departure_stop\",\n",
    "        \"arrival_time\",\n",
    "        \"arrival_stop\",\n",
    "        \"stop_pair\",\n",
    "    ]\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# There are some stop to stop connections one should never take such as riding a local train to get between far apart express stops.\n",
    "# We remove these artificially slower connections before proceeding to calculate the average stop-to-stop travel times.\n",
    "dep_arr_ranks = (\n",
    "    stop_to_stop.groupby(\"stop_pair\")[[\"departure_time\", \"arrival_time\"]]\n",
    "    .rank(\"min\")\n",
    "    .rename(\n",
    "        columns={\"departure_time\": \"sts_depart_rank\", \"arrival_time\": \"sts_arrive_rank\"}\n",
    "    )\n",
    ")\n",
    "stop_to_stop[\"sts_depart_rank\"] = dep_arr_ranks[\"sts_depart_rank\"]\n",
    "stop_to_stop[\"sts_arrive_rank\"] = dep_arr_ranks[\"sts_arrive_rank\"]\n",
    "stop_to_stop = stop_to_stop.sort_values(\"sts_arrive_rank\").reset_index(drop=True)\n",
    "drop_trips = stop_to_stop.groupby(\"stop_pair\")[\"sts_depart_rank\"].diff()\n",
    "while (drop_trips <= 0).sum() > 0:\n",
    "    stop_to_stop = stop_to_stop[~(drop_trips <= 0)]\n",
    "    drop_trips = stop_to_stop.groupby(\"stop_pair\")[\"sts_depart_rank\"].diff()\n",
    "\n",
    "# Here we calculate the expected wait time for each possible stop-to-stop trip.\n",
    "# This is equal to half the time between the last feasible departure and the next feasible departure for each stop-to-stop pair.\n",
    "stop_to_stop = stop_to_stop.reset_index(drop=True)\n",
    "stop_to_stop = stop_to_stop[\n",
    "    [\n",
    "        \"departure_time\",\n",
    "        \"departure_stop\",\n",
    "        \"arrival_time\",\n",
    "        \"arrival_stop\",\n",
    "        \"stop_pair\",\n",
    "        \"sts_depart_rank\",\n",
    "    ]\n",
    "]\n",
    "stop_to_stop[\"sts_depart_rank\"] = stop_to_stop.groupby(\"stop_pair\")[\n",
    "    \"departure_time\"\n",
    "].rank()\n",
    "previous_depart = stop_to_stop[\n",
    "    [\"stop_pair\", \"sts_depart_rank\", \"departure_time\"]\n",
    "].rename(columns={\"departure_time\": \"prev_departure_time\"})\n",
    "previous_depart[\"sts_depart_rank\"] = previous_depart[\"sts_depart_rank\"] + 1\n",
    "stop_to_stop = stop_to_stop.merge(\n",
    "    previous_depart, on=[\"stop_pair\", \"sts_depart_rank\"], how=\"left\"\n",
    ")\n",
    "stop_to_stop.loc[stop_to_stop[\"prev_departure_time\"].isna(), \"prev_departure_time\"] = (\n",
    "    str(transit_start_hr).zfill(2) + \":00:00\"\n",
    ")\n",
    "stop_to_stop[\"departure_time\"] = pd.to_datetime(\n",
    "    stop_to_stop[\"departure_time\"], format=\"%H:%M:%S\"\n",
    ")\n",
    "stop_to_stop[\"arrival_time\"] = pd.to_datetime(\n",
    "    stop_to_stop[\"arrival_time\"], format=\"%H:%M:%S\"\n",
    ")\n",
    "stop_to_stop[\"prev_departure_time\"] = pd.to_datetime(\n",
    "    stop_to_stop[\"prev_departure_time\"], format=\"%H:%M:%S\"\n",
    ")\n",
    "stop_to_stop[\"wait_time_secs\"] = (\n",
    "    stop_to_stop[\"departure_time\"] - stop_to_stop[\"prev_departure_time\"]\n",
    ").dt.total_seconds() / 2\n",
    "\n",
    "# Here we calculate the actual in transit time for each trip\n",
    "stop_to_stop[\"travel_time_secs\"] = (\n",
    "    stop_to_stop[\"arrival_time\"] - stop_to_stop[\"departure_time\"]\n",
    ").dt.total_seconds()\n",
    "\n",
    "# Here we add the expected wait time and the actual transit time for each trip to get the total travel time for each trip\n",
    "stop_to_stop[\"total_transit_time_secs\"] = (\n",
    "    stop_to_stop[\"wait_time_secs\"] + stop_to_stop[\"travel_time_secs\"]\n",
    ")\n",
    "\n",
    "# Now that we have calculated the total travel time for each possible stop-to-stop trip\n",
    "# we compute the average travel time for every feasible stop-to-stop pair during our parameter defined travel window\n",
    "stop_to_stop = (\n",
    "    stop_to_stop.groupby(\"stop_pair\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"departure_stop\": \"first\",\n",
    "            \"arrival_stop\": \"first\",\n",
    "            \"total_transit_time_secs\": \"median\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# We format these transit connections to match the source/target convention used by networkX\n",
    "stop_to_stop[\"mode\"] = \"mta\"\n",
    "stop_to_stop = stop_to_stop[\n",
    "    [\"departure_stop\", \"arrival_stop\", \"mode\", \"total_transit_time_secs\"]\n",
    "].rename(\n",
    "    columns={\n",
    "        \"departure_stop\": \"source\",\n",
    "        \"arrival_stop\": \"target\",\n",
    "        \"total_transit_time_secs\": \"weight\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the directed network of stop-to-stop mta connections where the weight of each edge is the average travel time (in seconds) for that stop-to-stop pair\n",
    "\n",
    "MTA_DG = nx.from_pandas_edgelist(\n",
    "    stop_to_stop, create_using=nx.DiGraph(), edge_attr=[\"weight\", \"mode\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stitch this directed transit network together with our other network of both walkable streets and (MTA stops)-to-(walkable streets) connections\n",
    "\n",
    "complete_DG = nx.compose(complete_DG, MTA_DG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have now finished creating our networkX walking and MTA NYC network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below we choose to store the parts of this network we want to keep as geodataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the entire network edgeset in one geodataframe which we will use in future vital parks scripts\n",
    "\n",
    "Vital_Parks_Edges = nx.to_pandas_edgelist(complete_DG)\n",
    "Vital_Parks_Edges = gpd.GeoDataFrame(Vital_Parks_Edges, geometry=\"geometry\").set_crs(\n",
    "    2263\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store the street-level nodes in a geodataframe because we want to keep the geo-location data for these particular nodes for the remaining Vital Parks Scripts\n",
    "\n",
    "Vital_Parks_Street_Nodes = (\n",
    "    pd.DataFrame([nx.get_node_attributes(complete_DG, \"geometry\")])\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"geometry\", \"index\": \"street_node_id\"})\n",
    ")\n",
    "Vital_Parks_Street_Nodes = gpd.GeoDataFrame(\n",
    "    Vital_Parks_Street_Nodes, geometry=\"geometry\"\n",
    ").set_crs(2263)\n",
    "\n",
    "# Restrict this set of street points so that we can only access the network at ground level (Level 'M').\n",
    "# This prevents us, for example, from 'entering' the street network in the middle of a bridge. We would only include the entrance nodes for this bridge and none of the higher midpoints.\n",
    "Vital_Parks_Street_Nodes = Vital_Parks_Street_Nodes[\n",
    "    Vital_Parks_Street_Nodes[\"street_node_id\"].str.contains(\"M\")\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################\n",
    "# Upload/save these edges and nodes geodataframes to SQL server, as a GeoJSON, or however you want to store it so that they can be accessed by the third vital parks script\n",
    "# ##################################################\n",
    "\n",
    "# Vital_Parks_Edges.to_file('Network_edges.geojson', driver='GeoJSON')\n",
    "\n",
    "# Vital_Parks_Street_Nodes.to_file('Network_nodes.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END Vital Parks Script Part 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
